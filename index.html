<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Intel(R) MKL-DNN: Developer Manual</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Developer Manual </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN) is an open source performance library for Deep Learning (DL) applications intended for acceleration of DL frameworks on Intel(R) architecture. Intel MKL-DNN includes highly vectorized and threaded building blocks for implementation of convolutional neural networks (CNN) with C and C++ interfaces. We created this project to help DL community innovate on Intel(R) processors.</p>
<p>The library supports the most commonly used primitives necessary to accelerate bleeding edge image recognition topologies, including AlexNet and VGG. The primitives include convolution, inner product, pooling, normalization and activation primitives with support for forward (scoring or inference) operations. Current release includes the following classess of functions:</p>
<ul>
<li>Convolution: direct batched convolution</li>
<li>Inner Product</li>
<li>Pooling: maximum</li>
<li>Normalization: local response normalization across channels (LRN)</li>
<li>Activation: rectified linear neuron activation (ReLU)</li>
<li>Data manipulation: reorder (multi-dimensional transposition/conversion)</li>
</ul>
<p>Intel(R) MKL DNN primitives implement a plain C application programming interface (API) that can be used in the existing C/C++ DNN frameworks, as well as in custom DNN applications.</p>
<h2>Programming Model</h2>
<p>In Intel MKL-DNN model primitives have other primitives as inputs. Outputs are memory primitives only. This makes it possible to reconstruct the graph of computations at the run time.</p>
<h3>Basic terminology</h3>
<p>Intel(R) MKL-DNN operates on the following three main objects:</p>
<ul>
<li><b>Primitive</b> - any operation: convolution, data format reorder, and even memory. Primitives can have other primitives as inputs but may have only memory primitives as outputs.</li>
<li><b>Engine</b> - an execution device. Currently the only supported engine is CPU. Every primitive is mapped to a specific engine.</li>
<li><b>Stream</b> - an execution context: user submits primitives to a stream and waits for their completion. Primitives submitted to a stream can have different engines. Stream also track dependencies between the primitives.</li>
</ul>
<p>The typical workflow is the following: create a set of primitives to be run, push them all at once or one-by-one into a stream and wait for the completion.</p>
<h3>Compute primitives</h3>
<p>To create a primitive, one has to first create a logical description of the operation such as a memory descriptor or a convolution descriptor. Then they need to create a description of the primitive to perform the operation with all the details like descriptions of inputs and outputs defined. Finally, they need to create a primitive specifying other primitives as inputs and outputs. The following list describes these levels of abstraction in details:</p>
<ul>
<li><b>Operation/memory descriptor</b> - a high level description with logical parameters of an operation/memory. For instance, for a convolution operation the description would contain parameters such as sizes, strides, propagation type, etc. A memory description would contain dimensions, precision and format of data layout in memory. The memory format may be set to <code>any</code>, which means that it is not yet defined. This is used to let primitives choose the memory format for optimal performance. This structure is lightweight and does not allocate any additional resources.</li>
<li><b>Primitive descriptor</b> - a complete description of a primitive containing an operation descriptor, descriptors of primitive inputs and outputs, and the target engine. In the future, it would be possible to user query a primitive descriptor for estimated performance, memory consumptions and so on. This is also a lightweight structure.</li>
<li><b>Primitive</b> - a specific instance of a primitive produced using a corresponding primitive descriptor. It contains pointers to input primitives and output memory. Creation of a primitive is a potentially expensive operation because when a primitive is created, MKL-DNN allocates resources that are needed to execute it.</li>
</ul>
<h3>Auxiliary types</h3>
<ul>
<li><b>Tensor</b> - an data description containing the number of dimensions the data has and the dimensions themselves.</li>
<li><b>Primitive_at</b> - a structure containing a primitive and an index. This structure specifies which output of the primitive to use as a input for another primitive.</li>
</ul>
<h3>C++ API example</h3>
<p>The repository contains an example how to build a block of neural network topology consisting of convolution, relu, lrn and pooling. Let's go through it step by step.</p>
<ol type="1">
<li>Initialize a CPU engine. The last parameter stands for the index of the engine. <div class="fragment"><div class="line"><span class="keyword">auto</span> cpu_engine = <a class="code" href="structmkldnn_1_1engine.html">mkldnn::engine</a>(<a class="code" href="structmkldnn_1_1engine.html#a81bcf1ea92d7f98852a2c3e187825de6a7e33e884a6d7eb40a73125557b14a7a7">mkldnn::engine::cpu</a>, 0);</div></div><!-- fragment --></li>
<li>Create a vector of primitives which will represent the net. <div class="fragment"><div class="line">std::vector&lt;mkldnn::primitive&gt; net;</div></div><!-- fragment --></li>
<li>Allocate input data and create a tensor structure that describes it. <div class="fragment"><div class="line">std::vector&lt;float&gt; src(2 * 3 * 227 * 227);</div><div class="line"><a class="code" href="structmkldnn_1_1tensor.html#a7a61d1a90eb1af9ce5ef6af16da938db">mkldnn::tensor::dims</a> conv_src_dims = {2, 3, 227, 227};</div></div><!-- fragment --></li>
<li>Create two memory desciptors: one for data in users' format, and one for the convolution input. We use <code>nchw</code> (minibatch-channels-height-width) format for user data format and wildcard <code>any</code> for the convolution data format. The wildcard means that convolution primitive will choose the data format that is most suitable for its input parameters (convolutional kernel sizes, strides, padding and so on). If the resulting format is different from <code>nchw</code>, user data will have to be transformed to the format expected by convolution. <div class="fragment"><div class="line"><span class="keyword">auto</span> user_src_md = <a class="code" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a>({conv_src_dims},</div><div class="line">    mkldnn::memory::precision::f32, mkldnn::memory::format::nchw);</div><div class="line"><span class="keyword">auto</span> conv_src_md = <a class="code" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a>({conv_src_dims},</div><div class="line">    mkldnn::memory::precision::f32, mkldnn::memory::format::any);</div></div><!-- fragment --></li>
<li>Create a convolution descriptor by specifying the algorithm, the propagation kind, shapes of input, weights, bias and output, convolution strides, padding and padding kind. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv_desc = <a class="code" href="structmkldnn_1_1convolution_1_1desc.html">mkldnn::convolution::desc</a>(</div><div class="line">    <a class="code" href="group__cpp__api__primitives.html#ggaeb087eae78f70a4d249a90aefa165cf8a064cbe5d8f2575433bfaaaef8937f369">mkldnn::prop_kind::forward</a>, <a class="code" href="structmkldnn_1_1convolution.html#a639185eef120eea1dffb4dc4fcbd748eabc7fccb84e69b918212c6df85ae02625">mkldnn::convolution::direct</a>,</div><div class="line">    conv_src_md, <span class="comment">/* format::any used here to let convolution choose a format */</span></div><div class="line">    conv_weights_md, conv_bias_md, conv_dst_md,</div><div class="line">    {1, 1}, {0, 0}, <a class="code" href="group__cpp__api__primitives.html#ggaa1ff931b28e0a90473ad5bddd21c60fcafbabef80df01bc0a5636d0ca9189355d">mkldnn::padding_kind::zero</a>);</div></div><!-- fragment --></li>
<li>Create a convolution primitive descriptor. Once created, the descriptor will have specific formats in place of any wildcard formats that were specified in the convolution descriptor. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv_pd = <a class="code" href="structmkldnn_1_1convolution_1_1primitive__desc.html">mkldnn::convolution::primitive_desc</a>(conv_desc, cpu_engine);</div></div><!-- fragment --></li>
<li>Create a memory primitive containing user's data and check whether user data format differs from what convolution expects. If yes, create a reorder primitive that transforms user data to the convolution format and add it to the net. <div class="fragment"><div class="line"><span class="keyword">auto</span> user_src_memory_descriptor</div><div class="line">    = <a class="code" href="structmkldnn_1_1memory_1_1primitive__desc.html">mkldnn::memory::primitive_desc</a>(user_src_md, engine);</div><div class="line"></div><div class="line"><span class="keyword">auto</span> user_src_memory = <a class="code" href="structmkldnn_1_1memory.html">mkldnn::memory</a>(user_src_memory_descriptor, src);</div><div class="line"></div><div class="line"><span class="comment">/* Check if we need a reorder */</span></div><div class="line"><span class="keyword">auto</span> conv_input = user_src_memory;</div><div class="line"><span class="keywordflow">if</span> (<a class="code" href="structmkldnn_1_1memory_1_1primitive__desc.html">mkldnn::memory::primitive_desc</a>(conv_pd.data.src_primitive_desc)</div><div class="line">        != user_src_memory_descriptor) {</div><div class="line">    <span class="comment">/* Yes, we do */</span></div><div class="line"></div><div class="line">    <span class="comment">/* Convolution primitive descriptor contains the memory primitive</span></div><div class="line"><span class="comment">     * descriptor it expects as it&#39;s input. Since we don&#39;t specify a</span></div><div class="line"><span class="comment">     * pointer to allocated memory, the mmory will be allocated by MKL-DNN */</span></div><div class="line">    <span class="keyword">auto</span> conv_src_memory = <a class="code" href="structmkldnn_1_1memory.html">mkldnn::memory</a>(conv_pd.data.src_primitive_desc);</div><div class="line"></div><div class="line">    <span class="comment">/* create a reorder between data, make it an input for a convolution */</span></div><div class="line">    conv_input = <a class="code" href="structmkldnn_1_1reorder.html">mkldnn::reorder</a>(user_src_memory, conv_src_memory)</div><div class="line"></div><div class="line">    <span class="comment">/* put the reorder in the net */</span></div><div class="line">    net.push_back(conv_input);</div><div class="line">}</div></div><!-- fragment --></li>
<li>Create a convolution primitive and add it to the net. <div class="fragment"><div class="line"><span class="comment">/* Note that the conv_input primitive (whether it is a memory or a reorder)</span></div><div class="line"><span class="comment"> * is an input dependency for the convolution primitive which means that the</span></div><div class="line"><span class="comment"> * convolution primitive won&#39;t be executed before the data is ready. */</span></div><div class="line"><span class="keyword">auto</span> conv = <a class="code" href="structmkldnn_1_1convolution.html">mkldnn::convolution</a>(conv_pd, conv_input, conv_weights_memory,</div><div class="line">        conv_user_bias_memory, conv_dst_memory);</div><div class="line">net.push_back(conv);</div></div><!-- fragment --></li>
<li>Finally, create a stream, submit all the primitives, and wait for completion. <div class="fragment"><div class="line"><a class="code" href="structmkldnn_1_1stream.html">mkldnn::stream</a>().<a class="code" href="structmkldnn_1_1stream.html#a018e38932cc072568a48015f55fbe798">submit</a>(net).<a class="code" href="structmkldnn_1_1stream.html#a577fe98393390dc1ba646266d57566e2">wait</a>();</div></div><!-- fragment --></li>
</ol>
<p><a class="el" href="legal_information.html">Legal Information</a> </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
